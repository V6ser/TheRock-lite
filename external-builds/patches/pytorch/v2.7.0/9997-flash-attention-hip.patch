--- a/aten/src/ATen/native/transformers/hip/flash_attn/flash_api.hip.cpp
+++ b/aten/src/ATen/native/transformers/hip/flash_attn/flash_api.hip.cpp
@@ -1,13 +1,13 @@
 // Copied from aten/src/ATen/native/transformers/cuda/flash_attn/flash_api.cpp

 #include <ATen/ATen.h>
 #include <ATen/core/Tensor.h>
-#include <ATen/cuda/CUDAContext.h>
-#include <ATen/cuda/CUDAGuard.h>
-#include <c10/cuda/CUDAStream.h>
+#include <ATen/hip/HIPContext.h>
+#include <ATen/hip/HIPGuard.h>
+#include <c10/hip/HIPStream.h>

-#include <ATen/native/transformers/cuda/flash_attn/flash_api.h>
+#include <ATen/native/transformers/hip/flash_attn/flash_api.h>

 namespace at {
 namespace native {
@@ -20,9 +20,9 @@
     bool is_causal,
     const c10::optional<at::Tensor>& attention_mask,
     c10::optional<double> scale) {
-  TORCH_CHECK(query.is_cuda(), "query must be a CUDA tensor");
-  TORCH_CHECK(key.is_cuda(), "key must be a CUDA tensor");
-  TORCH_CHECK(value.is_cuda(), "value must be a CUDA tensor");
+  TORCH_CHECK(query.is_hip(), "query must be a HIP tensor");
+  TORCH_CHECK(key.is_hip(), "key must be a HIP tensor");
+  TORCH_CHECK(value.is_hip(), "value must be a HIP tensor");
   TORCH_CHECK(
       query.dim() == 4, "query must be 4-dimensional");
   TORCH_CHECK(
@@ -50,12 +50,12 @@
     bool is_causal,
     const c10::optional<at::Tensor>& attention_mask,
     c10::optional<double> scale) {
-  TORCH_CHECK(grad_out.is_cuda(), "grad_out must be a CUDA tensor");
-  TORCH_CHECK(query.is_cuda(), "query must be a CUDA tensor");
-  TORCH_CHECK(key.is_cuda(), "key must be a CUDA tensor");
-  TORCH_CHECK(value.is_cuda(), "value must be a CUDA tensor");
-  TORCH_CHECK(out.is_cuda(), "out must be a CUDA tensor");
-  TORCH_CHECK(logsumexp.is_cuda(), "logsumexp must be a CUDA tensor");
+  TORCH_CHECK(grad_out.is_hip(), "grad_out must be a HIP tensor");
+  TORCH_CHECK(query.is_hip(), "query must be a HIP tensor");
+  TORCH_CHECK(key.is_hip(), "key must be a HIP tensor");
+  TORCH_CHECK(value.is_hip(), "value must be a HIP tensor");
+  TORCH_CHECK(out.is_hip(), "out must be a HIP tensor");
+  TORCH_CHECK(logsumexp.is_hip(), "logsumexp must be a HIP tensor");

   TORCH_CHECK(grad_out.dim() == 4, "grad_out must be 4-dimensional");
   TORCH_CHECK(query.dim() == 4, "query must be 4-dimensional");
@@ -75,18 +75,18 @@

 } // anonymous namespace

-TORCH_LIBRARY_IMPL(aten, CUDA, m) {
+TORCH_LIBRARY_IMPL(aten, HIP, m) {
   m.impl(
       "flash_attention_forward",
       dispatch(
-          c10::DispatchKey::CUDA,
+          c10::DispatchKey::HIP,
           TORCH_FN(at::native::_flash_attention_forward)));
   m.impl(
       "flash_attention_backward",
       dispatch(
-          c10::DispatchKey::CUDA,
+          c10::DispatchKey::HIP,
           TORCH_FN(at::native::_flash_attention_backward)));
 }

 } // namespace native
 } // namespace at
--- a/aten/src/ATen/native/transformers/hip/flash_attn/philox.cuh
+++ b/aten/src/ATen/native/transformers/hip/flash_attn/philox.cuh
@@ -41,18 +41,18 @@
 #pragma once

 #include <cstdint>
-#include <curand_kernel.h>
+#include <hiprand_kernel.h>
 #include <hip/hip_runtime.h>

 namespace at {
-namespace cuda {
+namespace hip {
 namespace philox {

-// We use the same Philox implementation as in cuRAND.
+// We use the same Philox implementation as in hipRAND.
 // This is Philox_4x32_10.
-// See /usr/local/cuda/include/curand_philox4x32_x.h
+// See /usr/local/hip/include/hiprand_philox4x32_x.h
 // This is the state of a single generator.
-using PhiloxCudaState = curandStatePhilox4_32_10_t;
+using PhiloxHipState = hiprandStatePhilox4_32_10_t;

 // The Philox algorithm generates 4 32-bit integers.
 // This is the output of a single generator.
@@ -71,8 +71,8 @@
 __device__ __constant__ const int PHILOX_ROUNDS = 10;

 // The Philox algorithm.
-// This is the same as in cuRAND.
-// See /usr/local/cuda/include/curand_philox4x32_x.h
+// This is the same as in hipRAND.
+// See /usr/local/hip/include/hiprand_philox4x32_x.h
 __device__ __forceinline__ result_type philox4x32_10_rounds(
     uint4 ctr,
     uint2 key) {
@@ -98,7 +98,7 @@

 // Initialize the Philox state.
 __device__ __forceinline__ void philox_initialize(
-    PhiloxCudaState* state,
+    PhiloxHipState* state,
     unsigned long long seed,
     unsigned long long offset) {
   state->d = 0;
@@ -115,7 +115,7 @@
 }

 // Generate 4 32-bit random integers.
-__device__ __forceinline__ result_type philox_gen(PhiloxCudaState* state) {
+__device__ __forceinline__ result_type philox_gen(PhiloxHipState* state) {
   result_type ret = state->output;
   if (state->d == 0) {
     ret = philox4x32_10_rounds(state->ctr, state->key);
@@ -141,6 +141,6 @@
 }

 } // namespace philox
-} // namespace cuda
+} // namespace hip
 } // namespace at

+
--- a/aten/src/ATen/native/transformers/hip/flash_attn/static_switch.h
+++ b/aten/src/ATen/native/transformers/hip/flash_attn/static_switch.h
@@ -1,5 +1,3 @@
-// Copied from aten/src/ATen/native/transformers/cuda/flash_attn/static_switch.h
 #pragma once

 #include <utility>
